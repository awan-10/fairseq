python -m fairseq_cli.interactive  ../downloads/en_moe_lm_15b/ --path ../downloads/en_moe_lm_15b/model.pt --task language_modeling --input input.txt  --is-moe  --distributed-world-size 2 --model-overrides "{'world_size': 2, 'moe_eval_capacity_token_fraction': 0.05}" --bpe gpt2
